import os
import json
import hashlib
import time
from pathlib import Path
from dotenv import load_dotenv
from google import genai
from app.utils import download_and_process_video

# --- FORCE LOAD .ENV ---
BASE_DIR = Path(__file__).resolve().parent.parent 
ENV_PATH = BASE_DIR / ".env"
load_dotenv(dotenv_path=ENV_PATH, verbose=True)

# --- CONFIGURATION ---
api_key = os.getenv("GEMINI_API_KEY")
print(f"üîë DEBUG: Loaded API Key? {'YES' if api_key else 'NO'}")

if not api_key:
    raise ValueError("CRITICAL: GEMINI_API_KEY not found in .env file.")

# Initialize Gemini Client
client = genai.Client(api_key=api_key)

# Initialize Redis
redis_url = os.getenv("REDIS_URL", "redis://localhost:6379")
try:
    import redis
    cache = redis.from_url(redis_url, decode_responses=True)
    cache.ping()
    print("‚úÖ Connected to Real Redis")
except:
    print("‚ö†Ô∏è Using Fake Redis (In-Memory)")
    from fakeredis import FakeRedis
    cache = FakeRedis(decode_responses=True)


async def analyze_video_logic(video_url: str):
    # 1. Cache Check
    video_id = hashlib.md5(video_url.encode()).hexdigest()
    
    if cached := cache.get(video_id):
        print(f"‚ö° Cache HIT: {video_url}")
        return json.loads(cached)

    print(f"üê¢ Cache MISS: {video_url}. Starting analysis...")

    video_path = None
    try:
        # 2. Download
        video_path = download_and_process_video(video_url)

        # 3. Upload
        print("Uploading to Gemini...")
        video_file = client.files.upload(file=video_path)

        # Poll for processing
        while video_file.state.name == "PROCESSING":
            time.sleep(2)
            video_file = client.files.get(name=video_file.name)

        if video_file.state.name == "FAILED":
            raise ValueError("Gemini failed to process video.")

        # 4. Analysis
        print("Running Analysis...")
        prompt = """
        You are an AI-Detector, a digital forensics AI. Analyze this video for deepfakes.
        Return JSON ONLY:
        {
            "Detector_score": int (0-100),
            "verdict": "Real" | "Fake",
            "forensics": { "visual_anomalies": [], "audio_anomalies": [] },
            "content_analysis": { "logical_flaws": [], "sentiment": "" }
        }
        """

        response = client.models.generate_content(
            model="gemini-2.5-flash",
            contents=[prompt, video_file]
        )
        
        raw_text = response.text.replace("```json", "").replace("```", "").strip()
        result = json.loads(raw_text)

        # 5. Cache
        cache.setex(video_id, 86400, json.dumps(result))
        return result

    except Exception as e:
        print(f"‚ùå Error: {e}")
        return {
            "Detector_score": 0, "verdict": "Error",
            "forensics": {"visual_anomalies": [str(e)]},
            "content_analysis": {}
        }
    finally:
        if video_path and os.path.exists(video_path):
            os.remove(video_path)


# --- NEW FUNCTION FOR TEXT ---
async def analyze_text_logic(text_content: str):
    """
    Analyzes raw text for AI generation patterns.
    """
    # 1. Cache Check (Hash the text content)
    text_id = hashlib.md5(text_content.encode()).hexdigest()
    
    if cached := cache.get(text_id):
        print(f"‚ö° Text Cache HIT")
        return json.loads(cached)

    print(f"üê¢ Text Cache MISS. Analyzing...")

    try:
        # 2. Gemini Analysis
        prompt = """
        You are an AI-Detector. Analyze this text to determine if it was generated by an AI/LLM.
        Look for: overly formal tone, lack of personal anecdote, repetitive sentence structure, and 'hallucination' patterns.

        Return JSON ONLY:
        {
            "Detector_score": int (0-100, where 100 is definitely AI),
            "verdict": "Human" | "AI-Generated" | "Mixed",
            "content_analysis": {
                "writing_style": "Formal/Casual/Robotic",
                "indicators": ["list of specific phrases or patterns found"]
            }
        }
        """

        response = client.models.generate_content(
            model="gemini-2.5-flash",
            contents=[prompt, text_content]
        )
        
        # Clean JSON
        raw_text = response.text.replace("```json", "").replace("```", "").strip()
        result = json.loads(raw_text)

        # 3. Cache Result
        cache.setex(text_id, 86400, json.dumps(result))
        return result

    except Exception as e:
        print(f"‚ùå Text Error: {e}")
        return {
            "Detector_score": 0, "verdict": "Error",
            "content_analysis": {"error": str(e)}
        }